---
title: "Predicting Uninsurance Status Using Supervised Machine Learning"
author: "Alicia Helfrich, Olivia Gomez, and Akbar Naqvi"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true
    code-overflow: wrap
    theme: journal
editor: visual
embed-resources: true
---

# Project Overview

## Introduction

The Patient Protection and Affordable Care Act (ACA), passed in 2010, increased access to health insurance coverage for over 20 million Americans. Yet in 2022, over 25 million Americans still did not have health insurance.[^1] Being uninsured has been shown to lead to poorer health outcomes, limited preventative care, and increased risk of mortality.[^2] Uninsurance is also associated with poor economic outcomes and increases general costs for employers, taxpayers, health systems, and the general public.[^3] For example, uncompensated care for those without insurance averaged over \$42 billion between 2015 and 2017.[^4]

[^1]: Tolbert, 2023

[^2]: National Immigration Law Center, 2003

[^3]: Davis, 2023

[^4]: Coughlin, 2021

Losing insurance can happen for a multitude of reasons, such as job loss or a change in eligibility for public insurance programs. However, getting people re-enrolled in insurance can be difficult due to lack of knowledge of available programs and financial assistance, high cost of insurance, and availability (e.g. coverage offered through employment).[^5] Due to the high social and economic costs associated with being uninsured, state and federal governments, non-profit organizations, and other interested stakeholders have launched campaigns to raise awareness on financial assistance programs, tax credits, state-based Marketplace exchanges, and Medicaid eligibility.[^6] Given that the uninsured population is made up of over 25 million people, targeting campaigns can save taxpayer dollars and increase the effectiveness of the programs.

[^5]: Tolbert, 2023

[^6]: Schwab, 2022

Given the negative consequences of insurances loss and the costs associated with re-enrolling individuals in insurance, we sought to develop a predictive model that would assist government in targeting and anticipating individuals that may be most as risk of losing their insurance. If effective, uch a predictive model can help state and federal policymakers and other interested stakeholders target outreach and enrollment activities.

## Methodology

We decided to develop a supervised machine learning model to predict the likelihood that someone may lose health insurance over the course of the year. Based on available literature and subject matter expertise, we know that uninsurance can be associated with characteristics such as geography, employment status, wealth, and demographics. Therefore, much of our analysis focused on high-quality datasets that contained some of these useful predictors.

## Data Sources

there were two key datasets that were utilized in our analysis: the Survey of Income and Program Participation and Medicaid Eligibility statistics.

### Survey of Income and Program Participation

The Survey of Income and Program Participation (SIPP) is an extensive survey conducted by the U.S. Census Bureau. It contains data on income, labor force participation, social program participation and eligibility, and general demographic characteristics. We are using the SIPP given its breadth, detail, and inclusion of health insurance status. We are using data from the year 2018, as we wanted to use data from before the COVID-19 pandemic. We chose to use 2018 versus 2019 due to limited observations in the 2019 dataset.

Survey participants provide data for each month in the preceding calendar year at the time of the interview. Because of this detail, observations are on the person-month level, meaning there could be 12 observations for a single person for a given variable. Household-level questions are copied across all members of the household.

The SIPP categorized health insurance into several broad categories: Private, Medicare, Medicaid, military, other health insurance coverage, and coverage under ACA (e.g. through insurance Marketplace exchanges, which were established by the ACA). Given a respondent's answer to the broad question regarding type of coverage, they are able to provide greater detail about where they received the plan.

#### Library Loadings

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(haven)
library(data.table)
library(readxl)
library(lubridate)
library(vcd)
library(caret)
library(tidymodels)
library(tune)
library(survey)
library(vip)

```

#### Reading in Data

```{r, cache=TRUE, cache.id="data_pull"}
#| message: false
#| warning: false

#initial pull of data
data_2018 <- fread("data/pu2018.csv", 
                   sep = "|",
                   showProgress = TRUE) %>% 
            select(-starts_with("A")) 


names(data_2018) <- toupper(names(data_2018))
#Make sure all the column names are upper-case
```

#### Creating Unique Respondent IDs

SIPP data is divided into annual "panels" with sub "waves", and sub "sample groups. Each observation in the data set represents a particular individual during a month. In order to analyze a loss or gain in insurance over the course of a year, we needed to create a unique identifier for each respondent. This was done through a combination of the SSUID, which was a subgroup number unique within a particular annual data, and PNUM, which was unique within each SSUID. The combination of SSUID and PNUM allowed us to assess insurance loss for each respondent and then eventually group observations by the individual.

```{r}
#| message: false
#| warning: false

data_2018 <- data_2018 %>% 
   #make unique ID for each respondent
  mutate(unique_id = paste(SSUID,PNUM))
  
```

#### Creating our outcome variable "insurance_gap"

The SIPP doesn't have an explicit variable that indicates if someone lost insurance throughout the calendar year. Therefore, we needed to use available data to generate a variable for "insurance_gap", which acted as our outcome variable. The insurance_gap variable is a bulean field, where "1" means that they had insurance the prior month and then didn't the following. A "0" would signify that someone either didn't have insurance for those two consecutive months, did have insurance, or gained insurance after not having it.

The process for creating this variable is detailed below:

```{r}
#| message: false
#| warning: false

#create a separate tibble that will eventually be joined back to SIPP data 
#only bringing in the details needed to calculate insurance loss - participant ID, month, and insurance status
#Insurance status is captured by "RHLTHMTH"

  lost_insurance_detail <- data_2018 %>%
    
    select(SPANEL,SSUID,SWAVE,PNUM,MONTHCODE,RHLTHMTH) %>% 
    mutate(unique_id = paste(SSUID,PNUM))

#creating variables to assess if someone lost insurance from one month to the next
lost_insurance_prep <- lost_insurance_detail %>% 
  
  #sort observations so that it is in order of individuals and then in in chronological order
  arrange(unique_id,MONTHCODE) %>% 
  group_by(unique_id) %>% 
    mutate(

      #recode health insurance field to be a new "no_insurance"
      no_insurance = case_when(
        RHLTHMTH == 2 ~ 1, #had no coverage whatsoever
        RHLTHMTH == 1 ~ 0 #had coverage
      ),
     
      #create field of "no_insurance_last_month" for each observation
      no_insurance_last_month = lag(no_insurance),
      
      #create variable subtracting no_insurance and no_insurance last month
      # values will = 1 when they had insurance last month,  but didn't this month
      lost_insurance = no_insurance - no_insurance_last_month,
      insurance_gap = case_when(
        lost_insurance > 0  ~ 1,
        lost_insurance <= 0 ~ 0),
     
      #rename month columns to make a clean cross-tabluar table
      month = case_when(
        MONTHCODE == 1 ~ "January",
        MONTHCODE == 2 ~ "February",
        MONTHCODE == 3 ~ "March",
        MONTHCODE == 4 ~ "April",
        MONTHCODE == 5 ~ "May",
        MONTHCODE == 6 ~ "June",
        MONTHCODE == 7 ~ "July",
        MONTHCODE == 8 ~ "August",
        MONTHCODE == 9 ~ "September",
        MONTHCODE == 10 ~ "October",
        MONTHCODE == 11 ~ "November",
        MONTHCODE == 12 ~ "December")
    )
  
      #create cross tabular table displaying a unique list of participants and
      #a "1" or a "0" in each month column to denote if they lost insurance last month
      insurance_gap <-  lost_insurance_prep %>% 
        pivot_wider(
      id_cols = unique_id, 
      names_from = month,
      values_from = insurance_gap) %>% 
        
        #add a new binary variable that counts the months someone lost insurance during the month
        mutate(
       month_of_insurance_loss = sum(January,February, March, April, May, June, July, August, September, October, November, December, na.rm = TRUE),
       
       #create dummy variable for "1" if someone lost insurance during the year
       # "0" if they didn't experience a new loss during the year
        insurance_gap = case_when(
          month_of_insurance_loss > 0 ~ 1,
          month_of_insurance_loss < 1 ~ 0)
        )
    
    #update the original data frame so that it is organized by unique individuals
      #The final dataset will keep demographic variables, etc from January/the first observation
      #But will add in the months that someone lost coverage and the new "insurance_gap" variable
      data_2018_final <- insurance_gap %>% 
        left_join(
          x = .,
          y = data_2018,
          by = "unique_id"
        ) %>% 
         distinct(unique_id, .keep_all = TRUE)
      
    
```

### State Medicaid Eligibility Data

Many individuals are insured with Medicaid, which can vary in terms of eligibility requirements across states and can change substantially over the course of the year. We a particular states' frequency with which eligibility checks are made, and other local Medicaid policy specifications, could be a useful predictor of insurance loss. (For example, three states allow for 12-month continuous eligibility, while others conduct periodic eligibility checks before the standard annual renewal date. To account for some of these variations, we have created several indicator variables.[^7][^8]). We compiled a small dataset that summarizes these characteristics and joined them to our main SIPP data based on the respondent's state of residence.

[^7]: Buettgens, 2023

[^8]: Brooks, 2020

```{r}
#| message: false
#| warning: false

# tibble of indicator variables for state eligibility rules of interest
state_eligibility_rules <- read_excel("data/state_eligibility_rules.xlsx")

# create new variables representing the type of eligibility rule that applies to the
  #state of residence of the respondent (as of January 2018)
data_2018_final <-  data_2018_final %>% 
  mutate( state_id = TEHC_ST) %>% 
        left_join(
          x = .,
          y = state_eligibility_rules,
          by = "state_id"
        )

```

## Exploratory Data Analysis

### Outcome Variable Exploration

As mentioned, we created the outcome variable insurance_gap, which indicates if the respondent lost coverage at any point over the course of a year. In looking at the distribution of this variable we found that only 696 out of over 63,000 individuals were estimated to have lost insurance, representing around 1% of the sample population.

```{r}
#| message: false
#| warning: false

summary(data_2018_final$insurance_gap)



data_2018_final %>%
  ggplot(mapping = aes(x = insurance_gap)) + 
  geom_bar(fill = "skyblue") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 3) +
  scale_x_continuous(breaks = c(0, 1)) +
  labs(title = "Most of the Sample is Insured, But There is Still Enough \nUninsured for Meaningful Analysis", y = "Count", x = "Coverage (0 = Covered, 1 = Lost Coverage)",
       subtitle = "Number of Respondents that Lost or Maintained Coverage") %>% 
  theme_minimal()



```

Given how low this metric appeared, decided to conduct a nonparametric binomial test to understand whether this estimate was statistically different than the insurance loss rate we would expect for the US population. Based on findings from this exercise, we adjusted our analysis accordingly.

-   The null hypothesis for our binomial test was that the proportion of individuals within our sample that lost insurance throughout the course of the year is equal to the instance rate of insurance loss for the broader population. The alternative was that these two proportions would not be equal. In order to properly conduct this test, we needed to understand the true rate of insurance loss for the US population over the course of a year to serve as our p value. Unfortunately, while there is much literature on how many individuals are uninsured within the course of a year, there are not readily available numbers on who many people experience the loss of insurance across a year. Therefore, we needed to make several assumptions based on the information available.

We were able to find that the [The Center on Budget and Policy Priorities](https://www.cbpp.org/research/health/closing-medicaid-coverage-gap-would-help-diverse-group-and-narrow-racial#_ftn6) reports 13% of middle-aged individuals that had Medicaid in January had experienced some kind of a gap in coverage throughout the year. Using this as an anchor, we would expect that this proportion would be lower for the general population. Those on private insurance, which make up most cases (as we show below), are more likely to sector that is at low risk of turnover/loss of private insurance, or be more likely to have resources to purchase personal insurance in the event of losing thier job. We decided to assume, based on limited data, that the true probability of losing insurance for our population of interest would be 10%.

```{r}

# Count the number of successes (individuals who lost insurance)
successes <- data_2018_final %>% 
  #those that lost insurance
  filter(insurance_gap == 1) %>% 
  #those that had it in January
  filter(RHLTHMTH == 1) %>% 
  nrow()

# Total number of observations (sample size)
total <- data_2018_final %>% 
  filter(insurance_gap == 1 | insurance_gap == 0) %>% 
   filter(RHLTHMTH == 1) %>% 
  nrow()

# Hypothesized probability of success (under the null hypothesis)
p_null <- 0.1

# Perform the exact binomial test
binom_test <- binom.test(x = successes, n = total, p = p_null,conf.level = .9)

# Print the results
print(binom_test)


```

Based on the p-value and t-statistic, there is sufficient evidence to reject the null hypothesis at the 𝛼=0.1 significance level. It is likely that the proportion of insurance loss for our sample and the broader US population is different. Based on our own subject matter knowledge, we believe that the true percentage that lost insurance should be higher than what our sample is representing.

These results raised several flags for us, and we took actions to attempt to mitigate our concerns. It is possible that the raw, unweighted sample, is under representing demographic groups that would be most likely to lost insurance. For instance, it is possible that the raw, unweighted sample, is under representing demographic groups that would be most likely to loss insurance, and therefore deflating this metric. We have opted to utilizing replicate weights available by Census within our model fitting process to try and account for this discrepancy.

Secondly, with a low rate of insurance loss within our sample, we understand that there may be a high likelihood of overfitting our model. We will assess our model against several error metrics, to interpret the utility of our model and within the model selection phase. This is expanded on within our model development section further.

### Examining Variable Related to Insurance Loss

We sought to examine several variables that we believe have a relevance to health insurance loss: food insecurity and private vs. public health insurance

#### Food Security Score

Food security can be an important predictor of uninsurance. One study found that compared to people who are food secure, individuals with food insecurity "postpone needed medical care and medications more often, use more emergency and inpatient care, and have higher care expenditures by as much as 121%.[^9]

[^9]: Sonik, 2019

The below visualization shows the distribution of food security in our sample. We found that the high majority of respondents identified as having high food security. A simplistic inference could be that any "risks" associated with low food security and uninsurance were less present in our sample population.

```{r}
#| message: false
#| warning: false

predictor_eda <- data_2018_final %>%
  select(
    EORIGIN, ECITIZEN,#country of origin
         EHOWWELL,ERACE,ESEX,#English proficiency,race,sex
         RFOODS, #food security
         RDIS, #disability status
         EMS, #marital status
  ) %>%
  summary()
  
predictor_eda

data_2018_final %>%
  group_by(RFOODS) %>%
  ggplot(mapping = aes(x = RFOODS)) + 
  geom_bar(fill = "skyblue") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 3) +
  scale_x_continuous(breaks = c(1, 2, 3)) +
  labs(title = "Food Security Is High in Sample", subtitle = "A Score of 1 is High Food Security, a Score of 3 is Low Food Security", y = "Count", x = "Food Security Score") %>% 
  theme_minimal()
  

```

#### Insurance Type Coverage

Given the focus on health insurance, we were also interested to see the distribution of coverage types in this sample. We found that around 2/3rd of the insured population in the US had private health insurance through an employer. Given how many in our sample rely on their job for insurance, We could infer or hypothesize that employment variables within the dataset may play heavily into the predictors of "insurance_gap".

```{r}
#| message: false
#| warning: false

# Count occurrences of RPRIMTH equal to 1 for each unique ID
private_counts <- data_2018_final %>%
  filter(RPRIMTH == 1) %>%
  count(unique_id, name = "private_count") %>%
  nrow()

# Count occurrences of RPUBMTH equal to 1 for each unique ID
public_counts <- data_2018_final %>%
  filter(RPUBMTH == 1) %>%
  count(unique_id, name = "public_count") %>%
  nrow()

counts_df <- data.frame(Coverage_Type = c("Private", "Public"),
                        Count = c(private_counts, public_counts))

ggplot(counts_df, aes(x = Coverage_Type, y = Count, fill = Coverage_Type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Count), vjust = -0.5, color = "black", size = 3) +
  labs(x = "Coverage Type", y = "Count") +
  scale_fill_manual(values = c("Private" = "skyblue", "Public" = "lavender")) +  
  labs(title = "Private Insurance Outweighs Public Insurance in Sample", fill = "Coverage Type") +
  theme_minimal()

```

### Missingness & Imputation Methods

In additional exploring several variable of interest, we sought to understand missingness within our sample. Luckily, the CENSUS Bureau utilized and published documentation around their own data imputation sources. Given the extensive expertise of Census within SIPP data, we relied heavily on thier work to mitigate systematically missing data that would have tampered with our analysis results. Census imputation protocols were as follows:

The SIPP uses two imputation methods based on the assumption that data are missing at random within subgroups of the population. The imputation methods used are:

1.  Model-based Imputation: A process that involves estimating sequential models that predict values for a missing variable, conditional on demographic data, all other topic flags earnings data within the IRS and SSA.

2.  Sequential hot-deck imputation: This method matches a record with missing data to that of a donor with similar background characteristics

We observed, after reviewing documentation, that there were over 600 variables that were missing data entirely (whether it be due to separate universes or another cause), we dropped all variables with complete missingness.

```{r}
#| message: false
#| warning: false


# dropping variables where every value is missing
vars_with_missing <- colnames(data_2018_final)[apply(data_2018_final, 2, function(x) any(is.na(x)))]
vars_with_all_missing <- sapply(data_2018_final, function(col) all(is.na(col)))
vars_to_drop <- names(vars_with_all_missing)[vars_with_all_missing]

data_2018_final <- data_2018_final %>%
select(-one_of(vars_to_drop))

```

## Data Preparation for Model Creation

After exploratory analysis, we made several decisions to refine our data so that it would be best positions to create a robust model. Namely, the SIPP dataset comes with a large, computationally-costly, set of variables. We focus on taking out variables that would be either least likelihood to improve predictions or most likely to cause overfitting or bias.

### Identifying Zero Variance Predictors

First, we focused on removing variables that had almost zero variance, which would limit their utility in predicting "insurance_gap". We compiled a list of zero variance columns through the code below:

```{r, cache=TRUE, cache.id="zero_variance_variables"}
#| message: false
#| warning: false

near_zero_cols <- names(data_2018_final)[nearZeroVar(x = data_2018_final, freqCut = 99.5/.5)]


```

### Identifying Highly Associated Variables

After identifying near-zero variance columns, we sought to remove predictors that were either perfectly correlated with insurance_gap or that would likely lead to overfitting of our data (Ex:in-depth questions around specific public or private health insurance, uninsurance, etc). We utilized logical reasoning to sift through the hundreds of health insurance or employment questions that we believed would overfit our model.

#### Removing Perfectly Correlated Variables

Secondly, we built a correlation matrix to identify variables that were perfectly correlated with insurance_gap. Below, we added in variables that either based on literature or subject matter knowledge that we believed would be most likely to be highly correlated with the variable.

Please note: although we are, in the case below, investigating the relationship between two categorical variables (versus continuous), we found the exercise useful for exploratory purposes and for balancing the computational costs of other tests of association.

```{r, cache=TRUE, cache.id="correlation_matrix"}
#| message: false
#| warning: false


# evaluating correlation
potential_predictors <- data_2018_final %>%
  ungroup() %>% 
  select(-all_of(near_zero_cols),
         # all employment data
         starts_with("EJB"),
         
         # medicaid eligibility
         starts_with("elig_"),
         
         #adult wellbeing
         starts_with("EAWB"),
         
         #food security
         starts_with("EFOOD"),
         
         #demographics
         TAGE_EHC,#Age
         RFAMKIND,#family size
         RFRELU18,RHNUM65OVER,
         EORIGIN,EMS,ECITIZEN,#country of origin
         EHOWWELL,ERACE,ESEX,#engligh proficiency,race,sex
         RFOODS, #food security
         RDIS, #disability status
         EMS, #marital status
         
         #personal finances
         TDEBT_AST, #personal debt
         THINC_AST,#household income
         TNETWORTH, #net worth
         
         #location
         EAWBSAFE, #safety
         EVOUCHER, #housing voucher
         TEHC_ST, #state
         TMETRO_INTV, #metropolitan area
  )

#narrow list to numeric predictors to allow for correlation matrix
potential_predictors_num <- potential_predictors %>% 
  select(where(is.numeric))

# Calculate correlation matrix
correlation_matrix <- cor(potential_predictors_num,use = "pairwise.complete.obs")


# Filter correlations to only show associations with insurance_gap
insurance_gap_correlations <- correlation_matrix["insurance_gap", ]

# Sort correlations in descending order, to later filer
significant_correlations <-  insurance_gap_correlations[order(abs(insurance_gap_correlations),decreasing = TRUE)] 

#Take the top 200 correlations to include within the model
significant_correlations <- 
    significant_correlations[abs(significant_correlations) == 1] 

# Convert the vector created in the prior step to a tibble
significant_correlations <- enframe(significant_correlations,name = "variable",value = "correlation_level") 

#view(significant_correlations)

```

There were 8 variables that were found to be perfectly associated with insurance_gap, namely dealing with employment metrics. Many of the variables, removed in the code below, related to employment/unemployment spells.

### Removing Other Highly Correlated

In addition to removing perfectly correlated variables with insurance_gap, we also decided to remove other variables that could cause overfitting of our model such as supplemental questions regarding health insurance, and other identified employment variables.

```{r}
data_2018_final_rf <- data_2018_final %>%
  ungroup() %>% 
  #remove zero variance variables
  select(-all_of(near_zero_cols),
  
  ## dropping health insurance variables
  -starts_with("EHI1WHO"),
   -starts_with("EHICOST"),
   -starts_with("EHIUNT"),
   -starts_with("EHLTHSAV"),
   -starts_with("ELIFEMNYN"),
   -starts_with("ELIFE"),
   -starts_with("EMCPART"),
   -starts_with("EMC_BMONTH"),
   -starts_with("EMDEND"),
   -starts_with("EMDEXCH"),
   -starts_with("EMDMTH"),
   -starts_with("EMD"),
   -starts_with("EMHI")
   -starts_with("EMILIEND"),
   -starts_with("EMLMTH"),
   -starts_with("EMLUNT"),
   -starts_with("EML_"),
   -starts_with("ENO"),
   -starts_with("EMO"),
   -starts_with("EOTH"),
   -starts_with("EOT_"),
   -starts_with("EOWN"),
   -starts_with("EPR"),
   -starts_with("EVA"),
    -starts_with("EWH"),
    -starts_with("EYNO"),
   -starts_with("RCDMTH"),
   -starts_with("RDIRECTANN"),
   -starts_with("REMPLOYANN"),
   -starts_with("RHI"),
   -starts_with("RMARKTPLACE"),
   -starts_with("RMC_"),
   -starts_with("RMD_"),
   -starts_with("RML_"),
   -starts_with("RMEDCARE"),
   -starts_with("RNONHH"),
   -starts_with("ROTHCOVMTH"),
   -starts_with("ROT_CONTFLG"),
   -starts_with("RPR"),
   -starts_with("RPRI"),
   -starts_with("RPUB"),
   -starts_with("RTRIC"),
   -starts_with("RVACARE"),
   -starts_with("TCVAL_MED"),
   -starts_with("TCVAL_STD"),
   -starts_with("TDIS10_CFLG"),
   -starts_with("TDIS1AMT"),
   -starts_with("TDIS1_CFLG"),
   -starts_with("TDOCNUM"),
   -starts_with("TDOCNUM_"),
   -starts_with("TFVAL_"),
   -starts_with("THIPAY"),
   -starts_with("TLIFE"),
   -starts_with("TVA5"),
   -starts_with("TVIS"),
  
  # Dropping selected employment variables
  -EJB4_EMONTH, 
  -RJB4_CFLG,
-EJB4_ENDWK,
-EJB4_STRTJAN,
-EJB4_STARTWK,
-EJB3_OINCB,
-EJB4_JOBID,
-TJB4_HOURLY1,
-month_of_insurance_loss,
-EMHLOAN2SITE, 
-EJB4_CEXPAY, 
-EJB3_AWOPSM3, 
-EJB3_AWPCFL3, 
-EJB3_AWOPSW3, 
-EJB3_AWOPEW3, 
-TJB4_CXAMT,
-EJB4_CEXPAY, 
-EJB3_AWOPSM3, 
-EJB3_AWPCFL3, 
-EJB3_AWOPSW3, 
-EJB3_AWOPEW3, 
-TJB4_CXAMT,
-EJB4_INCPB, 
-EJB4_CEXPAY, 
-EJB4_PFTLOSS, 
-EJB3_AWOPSM3, 
-EJB3_AWPCFL3, 
-EJB3_AWOPSW3, 
-EJB3_AWOPEW3, 
-TJB4_EMPB, 
-TBSJ5VAL, 
-TBSJ5DEBTVAL, 
-TJB4_CXAMT, 
-TJB4_PRFTB,
          insurance_gap) %>% 
  
  #convert insurance gap to be a factor for random forest modeling
  mutate(insurance_gap = factor(insurance_gap))


```

### Adding Survey Weights to Our Data

```{r, cache=TRUE, cache.id="joining_weights"}
#| message: false
#| warning: false

#Read in the replicate-weight data. This dataset is small enough that most machines
#	can read the whole file into memory

rw_2018 <- fread("data/rw2018.csv", sep = "|", showProgress = TRUE) %>%
  select(SSUID, PNUM, repwgt0, monthcode) %>%
  mutate(unique_id = paste(SSUID,PNUM))
          ## add in a place to drop everything but the("SSUID","PNUM","MONTHCODE") and WPFINWGT (monthly code)
rw_2018 <- rw_2018 %>% 
  filter(monthcode == 1)

#Make sure all the column names are upper-
names(rw_2018) <- toupper(names(rw_2018))

names(rw_2018) <- c(names(rw_2018)[names(rw_2018) != "UNIQUE_ID"], "unique_id")

#Merge primary data and replicate weights on SSUID, PNUM, MONTHCODE, SPANEL, and SWAVE
data_2018_final_rf <- inner_join(data_2018_final_rf, rw_2018, by = "unique_id")

```

## Model Development

For our supervised machine learning model, we decided to develop a random forest model. This algorithm was selected for several. First, given the level of nuance that goes into obtaining or losing insurance (ex: Medicaid varying from state to state), we felt that the model could capture underlying patterns in our data with a high level of accuracy. Additionally, our team experienced a number of hurdles when running computationally heavy code. While a random forest can be costly, it can be highly effective with relatively little hyper parameter tuning, and provides options for parallelization.

### Creating Training/Testing and Cross Validation

```{r}
#| message: false
#| warning: false

set.seed(20240430)

# create a split object
data_split <- initial_split(data = data_2018_final_rf, prop = 0.75)

# create the training and testing data
training <- training(x = data_split)
testing  <- testing(x = data_split)

# making less folds to reduce computational cost
rf_folds <- vfold_cv(data = training, v = 5)

```

### Feature Engineering

Given the advantages of a random forest model, we kept feature engineering to a minimum, namely removing nominal/categorical variables that had little predictive advantage (such as a participants ID number).

```{r}
#| message: false
#| warning: false


#pre process data
recipe <- recipe(insurance_gap ~., training) %>%
  update_role(unique_id,SSUID.x, SSUID.y, PNUM.x, PNUM.y,
              ENJ_LAYOTYP,
              EJB4_BSLRYB,TJB3_GAMT1,TSUR10AMT,TJB3_WKLY1,TJB1_OTHER3,
              EJB2_AWOPSM3,EJB2_AWOPEM3, EJB2_AWPCFL3, EJB2_AWOPSW3,EJB2_AWOPEW3,
              EJB2_AWOPRE3,TJB2_HOURLY3,TJB3_ANNSAL2,
              TJB3_IMBMIC,TJB3_OTHER1,
              new_role = "id") %>%
  step_rm(has_role("id")) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors())

#see the engineered training data
#baked_data <- bake(prep(recipe, training = training), new_data = training)

 
```

### Hyperparameter Tuning

We tested only the trees parameter given computational costs, but assigned min_n and mtry based on best practices for the amount of variables and observations within each fold.

```{r}
#| message: false
#| warning: false

# creating random forest tuning grid
rf_grid <- grid_regular(
  trees(range = c(100, 500)), levels = 2)

  # we selected these hyperparameters with tuning and cross validation
model <- rand_forest(
  trees = tune(),
  
  # taking roughly the square root of number of variables 
  mtry = 37,
  
   #taking 5-10% of total observations in a fold/in testing to avoid over fitting
  min_n = 300,
) %>%
  
  #setting parallelization to reduce computational cost
  set_mode(mode = "classification") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model) 

```

### Fitting Our Model

```{r, cache=TRUE, cache.id="initial_model_fitting"}
#| message: false
#| warning: false

resamples <- workflow %>%
  tune_grid(
    resamples = rf_folds,
    grid = rf_grid,
    metrics = metric_set(recall, precision),
    weights = REPWGT0
  )

results <- resamples %>%
  collect_metrics() 

results


model_best <- resamples %>%
 select_best() 


```

```{r, cache=TRUE, cache.id="final_model_fitting"}


# extract parameters of the best model
final_workflow <- 
  workflow %>% 
  finalize_workflow(model_best)

# apply the best specifications to the other "folds" of the data
final_model_fit <- 
  final_workflow %>% 
  parsnip::fit(data = training, weights = REPWGT0) 

```

## Analyzing Results and Evaluating Final Model

# Extract predictions and actuals from resamples

```{r}

# predict on the testing data

predictions <- bind_cols(
  testing,
  predict(
      object = final_model_fit, new_data = testing)
  )

```

```{r}


# Create the confusion matrix

confusion_matrix <- conf_mat(predictions, truth = insurance_gap, estimate = .pred_class)


confusion_matrix

```

The model correctly identified all true negatives

### Assessing Error

We are utilizing a confusion matrix to show the false and true positives and negative rates. For the purpose of our analysis, it is more important to minimize the false negatives, as missing people who had an insurance gap would be more problematic than identifying people who did not have such a gap as having the gap. In general, our philosophy is that if this model is an input on Medicaid eligibility, it is more desirable to be more generous. Thus, we put most emphasis on the recall of our model, as an evaluation of its performance.

### Variable Importance

```{r}

final_model_fit %>%
  extract_fit_parsnip() %>% 
  vip(num_features = 20) 
```

The month when a person began a job is the most important predictor. We hypothesize this may be because private insurance is closely related to employment. A new job may mean that an uninsured individual gains coverage as well.

Several of the next most important predictors are also variables related to employment, further confirming our earlier predictions of the importance of employment to health insurance. Age, location, and income are also important predictors.

## Conclusions

This model, while not perfect, could help inform policymakers at the state, federal, and local levels where to focus fiscal and legislative priorities to improve health insurance coverage and the overall public health of communities. By being able to predict a citizen's loss of coverage, programs could target the root causes of health insurance loss, like job loss. While stopgaps do exist for people who lose employment, and thus insurance, the coverage is expensive and poor in many places. Improving insurance coverage for unemployed populations could reduce overall healthcare spending and improve the health and well-being of communities.

This model could also inform state Medicaid agencies efforts to reduce unnecessary losses in coverage. As the Medicaid program is considered a safety net program, ensuring that everyone that is eligible for coverage has access is important. State agencies could target populations that have many uninsured individuals, as an example to how this model could be utilized.

## Limitations

There were a number of limitations to our analysis that should influence the interpretation of our results:

-   **Low instance of insurance loss within sample puts model at risk of over fitting.** Ultimately, the low instance of insurance_gap within our sample would warrant further investigation to ensure the available data within the SIPP is capturing loss accurately. It is possible that another dataset may have more valid health-specific information information.

-   **Participant recall bias.** Related to the above, SIPP data reports on monthly units, but often collects data at a quarterly or annual frequency. This would require individuals to recite, by memory their economic or health insurance status by month. This inherently could result in inaccurate reporting of insurance status and undermine the accuracy of our model.

-   **Changes in external factors that influence insurance loss limit generalizability.** There have been many macro economic or policy level changes that change how likely someone might be to losing insurance. For example, COVID 19 resulted in substantial changes to the economy and to health policy. During the pandemic many states were unable to adjust eligibility requirements for Medicaid, and recently have begun adjusting policy. These adjustments are already reporting in many individuals losing health coverage. These eligibility requirements would pay a large role in how many people lose insurance - but ultimately would not be integrated into our model. Updated data, both on eligibility and on an individual level, would need to be utilized to apply this model in a current setting.

-   **Sample population changes throughout sampling period.** For time constraints, demographic features of participants as of January were utilized to generate predictions on insurance loss. However, there are many attributes of a respondent that are subject to change and that could influence insurance loss. For example, an individual who lost Medicaid coverage because they moved to a new state with stricter Medicaid requirements, or an individual who became divorced and lost their spouses' insurance coverage. The omission of this data throughout our analysis means that useful patterns may not have been captured within our model, and therefore could hinder performance within other environments.

Ultimately, these factors, among others, should be considered prior to expanding on our model or utilizing to understanding broader, generalization predictions.

### Sources

Brooks, T., Roygardner, L., Artiga, S., Pham, O., & Dolan, R. (2020). *Medicaid and CHIP Eligibility, Enrollment, and Cost Sharing Policies as of January 2020: Findings from a 50-State Survey*. Kaiser Family Foundation (KFF). <https://www.kff.org/report-section/medicaid-and-chip-eligibility-enrollment-and-cost-sharing-policies-as-of-january-2020-findings-from-a-50-state-survey-tables/>

Buettgens, M. (2023). *Ensuring Continuous Eligibility for Medicaid and CHIP: Coverage and Cost Impacts for Adults*. The Commonwealth Fund. <https://www.commonwealthfund.org/publications/issue-briefs/2023/sep/ensuring-continuous-eligibility-medicaid-impacts-adults>

Coughlin, T. A., Samuel-Jakubos, H., & Garfield, R. (2021). *Sources of Payment for Uncompensated Care for the Uninsured*. Kaiser Family Foundation (KFF). <https://www.kff.org/uninsured/issue-brief/sources-of-payment-for-uncompensated-care-for-the-uninsured/>

Davis, K. (2003). *In the Literature: The Costs and Consequences of Being Uninsured*. <https://www.commonwealthfund.org/sites/default/files/documents/___media_files_publications_in_the_literature_2003_jun_the_costs_and_consequences_of_being_uninsured_davis_consequences_itl_663_pdf.pdf>

Schwab, R., Swindle, R., & Giovannelli, J. (2022). *State-Based Marketplace Outreach Strategies for Boosting Health Plan Enrollment of the Uninsured*. The Commonwealth Fund. <https://www.commonwealthfund.org/publications/issue-briefs/2022/oct/state-based-outreach-boosting-enrollment-uninsured>

Sonik, R. A. (2019). Health Insurance and Food Insecurity: Sparking a Potential Virtuous Cycle. *American Journal of Public Health*, *109*(9), 1163--1165. <https://doi.org/10.2105/AJPH.2019.305252>

*Survey of Income and Program Participation.* (2018). U.S. Census Bureau, retrieved from <https://www.census.gov/programs-surveys/sipp/data/datasets/2018-data/2018.html>

*The Consequences of Being Uninsured*. (2014). National Immigration Law Center. <https://www.nilc.org/wp-content/uploads/2015/11/consequences-of-being-uninsured-2014-08.pdf>

Tolbert, J., Drake, P., & Damico, A. (2023). *Key Facts about the Uninsured Population*. Kaiser Family Foundation (KFF). <https://www.kff.org/uninsured/issue-brief/key-facts-about-the-uninsured-population/>

---
title: "Predicting Uninsurance Status Using Supervised Machine Learning"
author: "Alicia Helfrich, Olivia Gomez, and Akbar Naqvi"
format: html
editor: visual
embed-resources: true
bibliography: references.bib  
---

## Project Overview

### Introduction

The Patient Protection and Affordable Care Act (ACA), passed in 2010, increased access to health insurance coverage for over 20 million Americans. Yet in 2022, over 25 million Americans still did not have health insurance.[^1] Being uninsured has been shown to lead to poorer health outcomes, limited preventative care, and increased risk of mortality.[^2] Uninsurance is also associated with poor economic outcomes and increases general costs for employers, taxpayers, health systems, and the general public.[^3] For example, uncompensated care for those without insurance averaged over \$42 billion between 2015 and 2017.[^4]

[^1]: Tolbert, 2023

[^2]: National Immigration Law Center, 2003

[^3]: Davis, 2023

[^4]: Coughlin, 2021

Losing insurance can happen for a multitude of reasons, such as job loss or a change in eligibility for public insurance programs. However, getting people re-enrolled in insurance can be difficult due to lack of knowledge of available programs and financial assistance, high cost of insurance, and availability (e.g. coverage offered through employment).[^5] Due to the high social and economic costs associated with being uninsured, state and federal governments, non-profit organizations, and other interested stakeholders have launched campaigns to raise awareness on financial assistance programs, tax credits, state-based Marketplace exchanges, and Medicaid eligibility.[^6] Given that the uninsured population is made up of over 25 million people, targeting campaigns can save taxpayer dollars and increase the effectiveness of the programs.

[^5]: Tolbert, 2023

[^6]: Schwab, 2022

### Methodology

As we described, high uninsurance rates can be expensive for both the federal government and state governments, and increase unnecessary morbidity and mortality. Given insurance is closely related to other factors in a person's life, including employment, income, race, gender, and family size, we wanted to utilize such factors to predict a person's likelihood of becoming uninsured over the course of a year. Having such information can help policymakers address underlying factors and plan ahead, fiscally and logistically.

Using supervised machine learning techniques and data from the Survey of Income and Participation (SIPP), we developed a model to predict the likelihood that a person would lose health insurance in the course of a year. Such a predictive model can help state and federal policymakers and other interested stakeholders target outreach and enrollment activities For example, it may be in a state's best interest to target enrollment campaigns for public insurance programs towards population groups of interest. As discussed, reducing the uninsured population can reduce health care costs for states and the federal government.

## Data Sources

### Survey of Income and Program Participation

The Survey of Income and Program Participation (SIPP) is an extensive survey conducted by the U.S. Census Bureau. It contains data on income, labor force participation, social program participation and eligibility, and general demographic characteristics. We are using the SIPP given its breadth, detail, and inclusion of health insurance status. We are using data from the year 2018, as we wanted to use data from before the COVID-19 pandemic. We chose to use 2018 versus 2019 due to limited observations in the 2019 dataset.

Survey participants provide data for each month in the preceding calendar year at the time of the interview. Because of this detail, observations are on the person-month level, meaning there could be 12 observations for a single person for a given variable. Household-level questions are copied across all members of the household.

The SIPP categorized health insurance into several broad categories: Private, Medicare, Medicaid, military, other health insurance coverage, and coverage under ACA (e.g. through insurance Marketplace exchanges, which were established by the ACA). Given a respondent's answer to the broad question regarding type of coverage, they are able to provide greater detail about where they received the plan.

The Medicaid program, the public health insurance program for low income individuals, varies heavily by state in terms of eligibility rules and guidelines. For example, three states allow for 12-month continuous eligibility, while others conduct periodic eligibility checks before the standard annual renewal date. To account for some of these variations, we have created several indicator variables.[^7][^8]

[^7]: Buettgens, 2023

[^8]: Brooks, 2020

#### Library Loadings

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(haven)
library(data.table)
library(readxl)
library(lubridate)
library(vcd)
library(caret)
library(tidymodels)
library(tune)
<<<<<<< HEAD
library(survey)
=======
library(vip)
>>>>>>> dfef010 (additions and edits)

```

### Reading in Data

```{r, cache=TRUE, cache.id="data_pull"}
#| message: false
#| warning: false

#initial pull of data
data_2018 <- fread("data/pu2018.csv", 
                   sep = "|",
                   showProgress = TRUE) %>% 
            select(-starts_with("A")) 


names(data_2018) <- toupper(names(data_2018))
#Make sure all the column names are upper-case
```

<<<<<<< HEAD
=======
### Weighting Data

https://www2.census.gov/programs-surveys/sipp/Select_approp_wgt_2014SIPPpanel.pdf

```{r}
#| message: false
#| warning: false

#Read in the replicate-weight data. This dataset is small enough that most machines
#	can read the whole file into memory

rw_2018 <- fread("data/rw2018.csv", sep = "|", showProgress = TRUE) %>%
  select(SSUID, PNUM, WPFINWGT, MONTHCODE) %>%  
  mutate(unique_id = paste(SSUID,PNUM))
          ## add in a place to drop everything but the("SSUID","PNUM","MONTHCODE") and WPFINWGT (monthly code)


names(rw_2018) <- toupper(names(rw_2019))
#Make sure all the column names are upper-


data_2018_final_rf <- left_join(data_2018_final_rf, rw_2018, by = c("unique_id", "SSUID", "PNUM", "MONTHCODE"))
#Merge primary data and replicate weights on SSUID, PNUM, MONTHCODE, SPANEL, and SWAVE



data_2018_final_rf <- svydesign(
  ids = ~unique_id,
  weights = ~WPFINWGT,
  data = data_2018_final_rf
)

```
>>>>>>> dfef010 (additions and edits)

### Creating Unique Respondent IDs

SIPP data is divided into annual "panels" with sub "waves", and sub "sample groups. Each observation in the data set represents a particular individual during a month. In order to analyze a loss or gain in insurance over the course of a year, we needed to create a unique identifier for each respondent. This was done through a combination of the SSUID, which was a subgroup number unique within a particular annual data, and PNUM, which was unique within each SSUID. The combination of SSUID and PNUM allowed us to assess insurance loss for each respondent and then eventually group observations by the individual.

```{r}
#| message: false
#| warning: false

data_2018 <- data_2018 %>% 
   #make unique ID for each respondent
  mutate(unique_id = paste(SSUID,PNUM))
  
```

### Insurance Loss Coding

The SIPP doesn't have an explicit variable that indicates if someone lost insurance, particularly on a month by month basis. Therefore, we needed to use available data to generate a variable for "insurance_gap", which is a dummy variable that denotes if an individual who started the year with insurance experienced a gap later in the year.

```{r}
#| message: false
#| warning: false

#create a separate tibble to calculate insurance gap, limiting to only participant IDs and 
#RHLTHMTH, which is the variable capturing if someone had any kind of insurance or not

lost_insurance_detail <- data_2018 %>%
  
  select(SPANEL,SSUID,SWAVE,PNUM,MONTHCODE,RHLTHMTH) %>% 
  mutate(unique_id = paste(SSUID,PNUM))

#creating variables to assess if someone lost insurance from one quarter to the next

lost_insurance_prep <- lost_insurance_detail %>% 
  
  #sort observations so that it is in order of individuals in chronological order
  arrange(unique_id,MONTHCODE) %>% 
  group_by(unique_id) %>% 
    mutate(

      #recode health insurance field to be a new "no_insurance"

      no_insurance = case_when(
        RHLTHMTH == 2 ~ 1, #had no coverage whatsoever
        RHLTHMTH == 1 ~ 0 #had coverage
      ),
     
      #create field of "no_insurance_last_month" for each observation
      no_insurance_last_month = lag(no_insurance),
      
      #create variable subtracting no_insurance and no_insurance last month

      # values will = 1 when they had insurance last month,  but didn't this month

      lost_insurance = no_insurance - no_insurance_last_month,
     
      insurance_gap = case_when(
        lost_insurance > 0  ~ 1,
        lost_insurance <= 0 ~ 0),
     
      #rename month columns to make a clean cross-tabluar table
      month = case_when(
        MONTHCODE == 1 ~ "January",
        MONTHCODE == 2 ~ "February",
        MONTHCODE == 3 ~ "March",
        MONTHCODE == 4 ~ "April",
        MONTHCODE == 5 ~ "May",
        MONTHCODE == 6 ~ "June",
        MONTHCODE == 7 ~ "July",
        MONTHCODE == 8 ~ "August",
        MONTHCODE == 9 ~ "September",
        MONTHCODE == 10 ~ "October",
        MONTHCODE == 11 ~ "November",
        MONTHCODE == 12 ~ "December")
    )
  
    


      #will use to filter the lost_medicaid_prep 

      insurance_gap <-  lost_insurance_prep %>% 
        pivot_wider(
      id_cols = unique_id, 
      names_from = month,
      values_from = insurance_gap) %>% 
        
        #add a new binary variable that counts the months somone lost insurance during the month
        mutate(
       month_of_insurance_loss = sum(January,February, March, April, May, June, July, August, September, October, November, December, na.rm = TRUE),
       
       #create dummy variable for "1" if someone lost insurance during the year
       # "0" if they didn't experience a new loss during the year
        insurance_gap = case_when(
          month_of_insurance_loss > 0 ~ 1,
          month_of_insurance_loss < 1 ~ 0)
        )
    
    #update the original data frame so that it is organized by unique individuals
      #The final dataset will keep demographic variables, etc from January/the first observation
      #But will add in the months that someone lost coverage and the new "insurance_gap" variable
      data_2018_final <- insurance_gap %>% 
        left_join(
          x = .,
          y = data_2018,
          by = "unique_id"
        ) %>% 
         distinct(unique_id, .keep_all = TRUE)
      
    
```

##Medicaid Eligibility Variables

```{r}
#| message: false
#| warning: false

# tibble of indicator variables for state eligibility rules of interest
state_eligibility_rules <- read_excel("data/state_eligibility_rules.xlsx")

# create new variables representing the type of eligibility rule that applies to the
  #state of residence of the respondent (as of January 2018)
data_2018_final <-  data_2018_final %>% 
  mutate( state_id = TEHC_ST) %>% 
        left_join(
          x = .,
          y = state_eligibility_rules,
          by = "state_id"
        )

```

## Exploratory Data Analysis

### Outcome Variable Exploration

We created the outcome variable insurance_gap, which indicates if the respondent lost coverage at any point over the course of a year.

```{r}
#| message: false
#| warning: false

summary(data_2018_final$insurance_gap)


data_2018_final %>%
  ggplot(mapping = aes(x = insurance_gap)) + 
  geom_bar(fill = "skyblue") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 3) +
  scale_x_continuous(breaks = c(0, 1)) +
  labs(title = "Insurance vs. Uninsurance in Data", y = "Count", x = "Coverage (0 = Covered, 1 = Lost Coverage)")

```

There are 696 cases of insurance loss found in the data. While small compared to the count of no insurance loss, it is still a large enough n that allows for further analysis. Additionally, this represents 1% of the total sample. If the assumption of broader generalization to the US population holds, 1% of the insurance holding population would be substantial enough of elicit a policy response.

We also wanted to explore the distributions of choice predictor variables of interest.

```{r}
#| message: false
#| warning: false

predictor_eda <- data_2018_final %>%
  select(
    EORIGIN, ECITIZEN,#country of origin
         EHOWWELL,ERACE,ESEX,#English proficiency,race,sex
         RFOODS, #food security
         RDIS, #disability status
         EMS, #marital status
  ) %>%
  summary()
  
predictor_eda

data_2018_final %>%
  group_by(RFOODS) %>%
  ggplot(mapping = aes(x = RFOODS)) + 
  geom_bar(fill = "skyblue") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, color = "black", size = 3) +
  scale_x_continuous(breaks = c(1, 2, 3)) +
  labs(title = "Food Security Score Distribution in Data", subtitle = "A Score of 1 is High Food Security, a Score of 3 is Low Food Security", y = "Count", x = "Food Security Score")
  

```

The above visualization shows us the distribution of food security in our sample. Food security can be an important predictor of uninsurance. One study found that compared to people who are food secure, individuals with food insecurity "postpone needed medical care and medications more often, use more emergency and inpatient care, and have higher care expenditures by as much as 121%.[^9]

[^9]: Sonik, 2019

Given the focus on health insurance, we were also interested to see the distribution of coverage types in this sample.

```{r}
#| message: false
#| warning: false

# Count occurrences of RPRIMTH equal to 1 for each unique ID
private_counts <- data_2018_final %>%
  filter(RPRIMTH == 1) %>%
  count(unique_id, name = "private_count") %>%
  nrow()

# Count occurrences of RPUBMTH equal to 1 for each unique ID
public_counts <- data_2018_final %>%
  filter(RPUBMTH == 1) %>%
  count(unique_id, name = "public_count") %>%
  nrow()

counts_df <- data.frame(Coverage_Type = c("Private", "Public"),
                        Count = c(private_counts, public_counts))

ggplot(counts_df, aes(x = Coverage_Type, y = Count, fill = Coverage_Type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = Count), vjust = -0.5, color = "black", size = 3) +
  labs(x = "Coverage Type", y = "Count") +
  scale_fill_manual(values = c("Private" = "blue", "Public" = "red")) +  
  labs(title = "Public vs. Private Insurance in Data", fill = "Coverage Type") +
  theme_minimal()

```

### Missingness & Imputation Methods

```{r}
#| message: false
#| warning: false


# dropping variables where every value is missing
vars_with_missing <- colnames(data_2018_final)[apply(data_2018_final, 2, function(x) any(is.na(x)))]
vars_with_all_missing <- sapply(data_2018_final, function(col) all(is.na(col)))
vars_to_drop <- names(vars_with_all_missing)[vars_with_all_missing]

data_2018_final <- data_2018_final %>%
select(-one_of(vars_to_drop))

```

The SIPP uses two imputation methods based on the assumption that data are missing at random within subgroups of the population. The imputation methods used are:

1.  Model-based Imputation: A process that involves estimating sequential models that predict values for a missing variable, conditional on demographic data, all other topic flags earnings data within the IRS and SSA.

2.  Sequential hot-deck imputation: This method matches a record with missing data to that of a donor with similar background characteristics

However, given that there were still over 600 variables with completely missing data (whether it be due to separate universes or another cause), we dropped all variables with complete missingness.

### Exploring Correlations with Insurance Gaps

In preparation for developing our predictive model, we decided to narrow down potential predictors, given the computational costs of fitting a model with over 2,100 variables (we experienced challenges processing both a correlation matrix and model fitting with this quantity). In response, we decided to use an intelligent approach, combining both data and subject matter expertise to narrow down to a reduced number of variables.

First, we used subject matter knowledge to select broader of variables that could be easily associated with an individual losing insurance and therefore likely to build a stronger predictive model.

-   **Employment Information**: Given that most individuals with health insurance have a private plan through their employer, we hypothesize that variables capturing employment would lend to a more effective model. For example - certain sectors may be volatile and therefore someone may be more likely to lose insurance within a given year. Note - in our process of review, we discovered that several variables were perfectly correlated with insurance loss, based on skip logic within the survey. These variables have been removed at the bottom of the following code chunk.
-   **State Medicaid Eligibility Policies** While many individuals have private insurance, another common type of coverage is Medicaid, which covers low-income individuals. As previously mentioned, eligibility requirements vary greatly from state to state. Therefore, it is likely that these underlying trends in eligibility may cause someone to be more or less likely to lose coverage if they lose employment or based on access to this public option. For this reason, we added the Medicaid and CHIP eligibility variables that we joined into our data.
-   **Demographic Features**: Given well documented health disparities in different racial, ethnic, and socioeconomic groups, we included these variables for the correlation analysis. Additionally, we opted to add in marital status and family size, given that families with spouses or more workers would have more options to continue coverage as a supplementary policy holder, even if losing their individual insurance.

```{r}
#| message: false
#| warning: false


# evaluating correlation
potential_predictors <- data_2018_final %>%
  ungroup() %>% 
  select(-all_of(near_zero_cols),
         # all employment data
         starts_with("EJB"),
         
         # medicaid eligibility
         starts_with("elig_"),
         
         #adult wellbeing
         starts_with("EAWB"),
         
         #food security
         starts_with("EFOOD"),
         
         #demographics
         TAGE_EHC,#Age
         RFAMKIND,#family size
         RFRELU18,RHNUM65OVER,
         EORIGIN,EMS,ECITIZEN,#country of origin
         EHOWWELL,ERACE,ESEX,#engligh proficiency,race,sex
         RFOODS, #food security
         RDIS, #disability status
         EMS, #marital status
         
         #personal finances
         TDEBT_AST, #personal debt
         THINC_AST,#household income
         TNETWORTH, #net worth
         
         #location
         EAWBSAFE, #safety
         EVOUCHER, #housing voucher
         TEHC_ST, #state
         TMETRO_INTV, #metropolitan area
  )

#narrow list to numeric predictors to allow for correlation matrix
potential_predictors_num <- potential_predictors %>% 
  select(where(is.numeric))

# Calculate correlation matrix
correlation_matrix <- cor(potential_predictors_num,use = "pairwise.complete.obs")


# Filter correlations to only show associations with insurance_gap
insurance_gap_correlations <- correlation_matrix["insurance_gap", ]

# Sort correlations in descending order, to later filer
significant_correlations <-  insurance_gap_correlations[order(abs(insurance_gap_correlations),decreasing = TRUE)] 

#Take the top 200 correlations to include within the model
significant_correlations <- 
    significant_correlations[abs(significant_correlations) == 1] %>% 
  print()

# Convert the vector created in the prior step to a tibble
significant_correlations <- enframe(significant_correlations,name = "variable",value = "correlation_level")

#take the names of variables and put into a separate tibble 
significant_correlations_names <- significant_correlations %>% 
  as_tibble() %>% 
  select(variable) %>% 
  pull(variable)

```

## Predictive Modeling

### Variable Selection

```{r, cache=TRUE, cache.id="zero_variance_variables"}
#| message: false
#| warning: false

near_zero_cols <- names(data_2018_final)[nearZeroVar(x = data_2018_final, freqCut = 99.5/.5)]

data_2018_final_rf <- data_2018_final %>%
  ungroup() %>% 
  #remove zero variance variables
  select(-all_of(near_zero_cols),
  
  ## dropping health insurance variables
  -starts_with("EHI1WHO"),
   -starts_with("EHICOST"),
   -starts_with("EHIUNT"),
   -starts_with("EHLTHSAV"),
   -starts_with("ELIFEMNYN"),
   -starts_with("ELIFE"),
   -starts_with("EMCPART"),
   -starts_with("EMC_BMONTH"),
   -starts_with("EMDEND"),
   -starts_with("EMDEXCH"),
   -starts_with("EMDMTH"),
   -starts_with("EMD"),
   -starts_with("EMHI")
   -starts_with("EMILIEND"),
   -starts_with("EMLMTH"),
   -starts_with("EMLUNT"),
   -starts_with("EML_"),
   -starts_with("ENO"),
   -starts_with("EMO"),
   -starts_with("EOTH"),
   -starts_with("EOT_"),
   -starts_with("EOWN"),
   -starts_with("EPR"),
   -starts_with("EVA"),
    -starts_with("EWH"),
    -starts_with("EYNO"),
   -starts_with("RCDMTH"),
   -starts_with("RDIRECTANN"),
   -starts_with("REMPLOYANN"),
   -starts_with("RHI"),
   -starts_with("RMARKTPLACE"),
   -starts_with("RMC_"),
   -starts_with("RMD_"),
   -starts_with("RML_"),
   -starts_with("RMEDCARE"),
   -starts_with("RNONHH"),
   -starts_with("ROTHCOVMTH"),
   -starts_with("ROT_CONTFLG"),
   -starts_with("RPR"),
   -starts_with("RPRI"),
   -starts_with("RPUB"),
   -starts_with("RTRIC"),
   -starts_with("RVACARE"),
   -starts_with("TCVAL_MED"),
   -starts_with("TCVAL_STD"),
   -starts_with("TDIS10_CFLG"),
   -starts_with("TDIS1AMT"),
   -starts_with("TDIS1_CFLG"),
   -starts_with("TDOCNUM"),
   -starts_with("TDOCNUM_"),
   -starts_with("TFVAL_"),
   -starts_with("THIPAY"),
   -starts_with("TLIFE"),
   -starts_with("TVA5"),
   -starts_with("TVIS"),
  -EJB4_EMONTH, 
  -RJB4_CFLG,
-EJB4_ENDWK,
-EJB4_STRTJAN,
-EJB4_STARTWK,
-EJB3_OINCB,
-EJB4_JOBID,
-TJB4_HOURLY1,
  
  #remove variables that are perfectly correlated with insurance gap
  #-all_of(significant_correlations_names),
          insurance_gap) %>% 
  mutate(insurance_gap = factor(insurance_gap))


```

The correlation matrix helped us narrow down on which predictors to use to predict a loss of insurance. We made sure to drop any variables that had a correlation of 1 or -1 with our outcome variable to ensure we did not have any "perfect predictors" present in our model. We noticed that many variables with strong correlations were related to labor force participation. We hypothesize this is because the largest source of private insurance in the United States is through an employer. Respondents with steady jobs are more likely to have a steady and consistent access to health insurance.

We dropped variables related to health insurance to ensure we do not have any perfect predictors of health insurance status. For example, we dropped a variable that indicates if a respondent pays a premium. This would be a perfect predictor of a respondent having health insurance. Thus, for clarity, we dropped all variables that do not indicate the type of coverage a respondent has, and if they are covered or not.

We also dropped all variables with zero variance.

We also included variables related to age, food security, marriage status, disability, and other factors. While some variables may not have strong correlation values, we still plan to include them as predictors in our model as we do not want to use correlation alone as a deciding factor in our predictor selection. We plan to explore variable importance following the development and application of our predictive model.

### Weighting Data

https://www2.census.gov/programs-surveys/sipp/Select_approp_wgt_2014SIPPpanel.pdf

```{r}

#Read in the replicate-weight data. This dataset is small enough that most machines
#	can read the whole file into memory

rw_2018 <- fread("data/rw2018.csv", sep = "|", showProgress = TRUE) %>%
  select(SSUID, PNUM, WPFINWGT, MONTHCODE) %>%  
  mutate(unique_id = paste(SSUID,PNUM))
          ## add in a place to drop everything but the("SSUID","PNUM","MONTHCODE") and WPFINWGT (monthly code)


names(rw_2018) <- toupper(names(rw_2019))
#Make sure all the column names are upper-


data_2018_final_rf <- left_join(data_2018_final_rf, rw_2018, by = c("unique_id", "SSUID", "PNUM", "MONTHCODE"))
#Merge primary data and replicate weights on SSUID, PNUM, MONTHCODE, SPANEL, and SWAVE



data_2018_final_rf <- svydesign(
  ids = ~unique_id,
  weights = ~WPFINWGT,
  data = data_2018_final_rf
)

```



### Creating Training/Testing and Cross Validation

```{r}
#| message: false
#| warning: false

set.seed(20240430)

# create a split object
data_split <- initial_split(data = data_2018_final_rf, prop = 0.75)


# create the training and testing data
training <- training(x = data_split)
testing  <- testing(x = data_split)

# making less folds to reduce computational cost
rf_folds <- vfold_cv(data = training, v = 5)

```

### Feature Engineering

```{r}
#| message: false
#| warning: false


#pre process data
recipe <- recipe(insurance_gap ~., training) %>%
  update_role(unique_id,state_name,SSUID,PNUM,ENJ_LAYOTYP,
              EJB4_BSLRYB,TJB3_GAMT1,TSUR10AMT,TJB3_WKLY1,TJB1_OTHER3,
              EJB2_AWOPSM3,EJB2_AWOPEM3, EJB2_AWPCFL3, EJB2_AWOPSW3,EJB2_AWOPEW3,EJB2_AWOPRE3,TJB2_HOURLY3,TJB3_ANNSAL2,
              TJB3_IMBMIC,TJB3_OTHER1,
              new_role = "id") %>%
  step_rm(has_role("id")) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) 

#see the engineered training data
#baked_data <- bake(prep(recipe, training = training), new_data = training)


 
```

# Hyperparameter Tuning

```{r}
#| message: false
#| warning: false

# creating random forest tuning grid
rf_grid <- grid_regular(
  trees(range = c(100, 500)), levels = 2)

  # we selected these hyperparameters with tuning and cross validation
model <- rand_forest(
  trees = tune(),
  
  # taking roughly the square root of number of variables 
  mtry = 37,
  
   #taking 5-10% of total observations in a fold/in testing to avoid over fitting
  min_n = 300,
  
  #add weights
  weights = WFFINWGT
) %>%
  
  #setting parallelization to reduce computational cost
  set_mode(mode = "classification") |>
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model) 



design <- svydesign(
  ids = ~SSUID + PNUM + MONTHCODE,
  weights = ~WPFINWGT,
  data = data_2018
)





```

### Fitting Our Model

```{r, cache=TRUE, cache.id="initial_model_fitting"}
#| message: false
#| warning: false

resamples <- workflow %>%
  tune_grid(
    resamples = rf_folds,
    grid = rf_grid,
    metrics = metric_set(recall, precision)
  )

results <- resamples %>%
  collect_metrics()


results


model_best <- resamples %>%
 select_best() 


```

```{r, cache=TRUE, cache.id="final_model_fitting"}


# extract parameters of the best model
final_workflow <- 
  workflow %>% 
  finalize_workflow(model_best)

# apply the best specifications to the other "folds" of the data
final_model_fit <- 
  final_workflow %>% 
  fit(data = training)

rf_final <- workflow |>
  last_fit() 

rf_final |>
  extract_fit_parsnip() |>
  vip(num_features = 20) 


```

## Analyzing Results and Evaluating Final Model

# Extract predictions and actuals from resamples

```{r}

# predict on the testing data

predictions <- bind_cols(
  testing,
  predict(
      object = final_model_fit, new_data = testing)
  )

```
=======
```{r}
# Extract predictions and actuals from resamples
predictions <- rf_resamples %>%
  collect_predictions()


# Create the confusion matrix

confusion_matrix <- conf_mat(predictions, truth = insurance_gap, estimate = .pred_class)


confusion_matrix

```
### Assessing Error

We are using a confusion matrix to depict the false positive/negative rate of our predictions. In our case, it is more desirable to minimize false negatives, as to not misidentify people who had an insurance gap to not have it, since this would deny them any assistance they've been entitled to, compared to predicting someone without an insurance gap has one, which would provide them assistance they should not be getting. Thus, our metric of choice will be recall, which looks at the percentage of those who were correctly predicted to have insurance as a percentage of all of those who actually had it.

<<<<<<< HEAD
=======
We display a confusion matrix, but ultimately decide to utilize model's recall in our metric as our main metric, since the cost of false negatives in our case is higher than the cost of false positives. Meaning, it is less desirable to falsely predict a person who had an insurance gap didn't have one, rather than predict that a person who did not have a gap had one. So, the measure of the proportion of correctly predicted individuals who had an insurance gap compared to individuals who actually had an insurance gap is a crucial metric to use in our model.

We are utilizing a confusion matrix to show the false and true positives and negative rates. For the purpose of our analysis, it is more important to minimize the false negatives, as missing people who had an insurance gap would be more problematic than identifying people who did not have such a gap as having the gap. In general, our philosophy is that if this model is an input on Medicaid eligibility, it is more desirable to be more generous. Thus, we put most emphasis on the recall of our model, as an evaluation of its performance.

### Assessing Error

-   precision versus accuracy - justify error metric - Akbar


>>>>>>> c16012d (addressing merge conflicts)
### Variable Importance

```{r}

final_model_fit %>%
  extract_fit_parsnip() %>% 
  vip(num_features = 20) 
```

Month of insurance loss was the most important predictor for the model. We hypothesize this may be because insurance enrollment for Marketplace and private coverage can only happen during specific periods of time, called an Open Enrollment period. Medicaid and Medicare are also effected by this, but not as strongly as private coverage (i.e., if you become eligible for Medicare by turning 65, you're immediately eligible and able to enroll). Open enrollment typically occurs at the end of the year. If someone loses coverage in February, for example, it's likely they will not have coverage if they are attempting to enroll in a private plan. 

Several of the next most important predictors are variables related to employment. This confirms our earlier predictions of the importance of employment to health insurance. Age and income are also important predictors.


## Conclusions

This model, while not perfect, could help inform policymakers at the state, federal, and local levels where to focus fiscal and legislative priorities to improve health insurance coverage and the overall public health of communities. By being able to predict a citizen's loss of coverage, programs could target the root causes of health insurance loss, like job loss. While stopgaps do exist for people who lose employment, and thus insurance, the coverage is expensive and poor in many places. Improving insurance coverage for unemployed populations could reduce overall healthcare spending and improve the health and well-being of communities. 

This model could also inform state Medicaid agencies efforts to reduce unnecessary losses in coverage. As the Medicaid program is considered a safety net program, ensuring that everyone that is eligible for coverage has access is important. State agencies could target populations that have many uninsured individuals, as an example to how this model could be utilized.

## Limitations

-   Seam mismatch
-   Data collection protocols - memory of respondents
-   Changes in insurance policy (medicaid changes)
-   macro level economics
-   Respondents moving to different states, changes throughout the year

### Sources

Brooks, T., Roygardner, L., Artiga, S., Pham, O., & Dolan, R. (2020). *Medicaid and CHIP Eligibility, Enrollment, and Cost Sharing Policies as of January 2020: Findings from a 50-State Survey*. Kaiser Family Foundation (KFF). <https://www.kff.org/report-section/medicaid-and-chip-eligibility-enrollment-and-cost-sharing-policies-as-of-january-2020-findings-from-a-50-state-survey-tables/>

Buettgens, M. (2023). *Ensuring Continuous Eligibility for Medicaid and CHIP: Coverage and Cost Impacts for Adults*. The Commonwealth Fund. <https://www.commonwealthfund.org/publications/issue-briefs/2023/sep/ensuring-continuous-eligibility-medicaid-impacts-adults>

Coughlin, T. A., Samuel-Jakubos, H., & Garfield, R. (2021). *Sources of Payment for Uncompensated Care for the Uninsured*. Kaiser Family Foundation (KFF). <https://www.kff.org/uninsured/issue-brief/sources-of-payment-for-uncompensated-care-for-the-uninsured/>

Davis, K. (2003). *In the Literature: The Costs and Consequences of Being Uninsured*. <https://www.commonwealthfund.org/sites/default/files/documents/___media_files_publications_in_the_literature_2003_jun_the_costs_and_consequences_of_being_uninsured_davis_consequences_itl_663_pdf.pdf>

Schwab, R., Swindle, R., & Giovannelli, J. (2022). *State-Based Marketplace Outreach Strategies for Boosting Health Plan Enrollment of the Uninsured*. The Commonwealth Fund. <https://www.commonwealthfund.org/publications/issue-briefs/2022/oct/state-based-outreach-boosting-enrollment-uninsured>

Sonik, R. A. (2019). Health Insurance and Food Insecurity: Sparking a Potential Virtuous Cycle. *American Journal of Public Health*, *109*(9), 1163â€“1165. <https://doi.org/10.2105/AJPH.2019.305252>

*Survey of Income and Program Participation.* (2018). U.S. Census Bureau, retrieved from <https://www.census.gov/programs-surveys/sipp/data/datasets/2018-data/2018.html>

*The Consequences of Being Uninsured*. (2014). National Immigration Law Center. <https://www.nilc.org/wp-content/uploads/2015/11/consequences-of-being-uninsured-2014-08.pdf>

Tolbert, J., Drake, P., & Damico, A. (2023). *Key Facts about the Uninsured Population*. Kaiser Family Foundation (KFF). <https://www.kff.org/uninsured/issue-brief/key-facts-about-the-uninsured-population/>

